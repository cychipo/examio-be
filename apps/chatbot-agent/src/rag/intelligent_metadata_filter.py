"""
Intelligent Metadata Filtering System
Káº¿t há»£p hard matching, semantic matching vÃ  fallback mechanism
"""

import re
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain_core.documents import Document

class IntelligentMetadataFilter:
    def __init__(self, similarity_threshold: float = 0.7):
        """
        Args:
            similarity_threshold: NgÆ°á»¡ng similarity cho semantic matching (0.0-1.0)
        """
        self.similarity_threshold = similarity_threshold
        self.embedding_model = None
        self.department_embeddings = None
        self.department_list = None
        self._init_semantic_model()
    
    def _init_semantic_model(self):
        """Initialize semantic model for soft matching"""
        try:
            # Sá»­ dá»¥ng model nháº¹ cho tiáº¿ng Viá»‡t
            self.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
        except:
            try:
                # Fallback model
                self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            except:
                print("âš ï¸  Cannot load embedding model for semantic filtering")
                self.embedding_model = None
    
    def normalize_query(self, query: str) -> str:
        """Chuáº©n hÃ³a query cá»§a ngÆ°á»i dÃ¹ng"""
        if not query:
            return ""
        
        # Chuyá»ƒn vá» lowercase
        normalized = query.lower().strip()
        
        # Xá»­ lÃ½ cÃ¡c tá»« viáº¿t táº¯t phá»• biáº¿n
        abbreviations = {
            'attt': 'an toÃ n thÃ´ng tin',
            'cntt': 'cÃ´ng nghá»‡ thÃ´ng tin', 
            'kt': 'kinh táº¿',
            'qtdn': 'quáº£n trá»‹ doanh nghiá»‡p',
            'tc': 'tÃ­n chá»‰',
            'Ä‘bclgd': 'Ä‘áº£m báº£o cháº¥t lÆ°á»£ng giÃ¡o dá»¥c'
        }
        
        for abbr, full_form in abbreviations.items():
            normalized = re.sub(f'\\b{abbr}\\b', full_form, normalized)
        
        # Chuáº©n hÃ³a khoáº£ng tráº¯ng
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # XÃ³a dáº¥u cÃ¢u khÃ´ng cáº§n thiáº¿t
        normalized = re.sub(r'[^\w\sÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ä‘Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µ]', ' ', normalized)
        
        return normalized.strip()
    
    def extract_keywords_from_query(self, normalized_query: str) -> List[str]:
        """TrÃ­ch xuáº¥t keywords tá»« normalized query"""
        # Danh sÃ¡ch stop words tiáº¿ng Viá»‡t
        stop_words = {
            'cá»§a', 'lÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'vá»', 'cho', 'tá»«', 'vÃ ', 'hoáº·c', 
            'cÃ¡c', 'má»™t', 'nÃ y', 'Ä‘Ã³', 'khi', 'vá»›i', 'Ä‘á»ƒ', 'hay', 'nhá»¯ng', 'sáº½',
            'nhÆ°', 'thÃ¬', 'táº¡i', 'trÃªn', 'dÆ°á»›i', 'giá»¯a', 'bÃªn', 'ngoÃ i', 'sau',
            'trÆ°á»›c', 'theo', 'báº±ng', 'qua', 'ra', 'vÃ o', 'lÃªn', 'xuá»‘ng'
        }
        
        # TÃ¡ch tá»« vÃ  lá»c stop words
        words = normalized_query.split()
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # TrÃ­ch xuáº¥t cá»¥m tá»« cÃ³ Ã½ nghÄ©a (bigrams, trigrams)
        meaningful_phrases = []
        
        # Bigrams
        for i in range(len(words) - 1):
            if words[i] not in stop_words and words[i+1] not in stop_words:
                meaningful_phrases.append(f"{words[i]} {words[i+1]}")
        
        # Trigrams cho cÃ¡c cá»¥m tá»« tá»• chá»©c
        for i in range(len(words) - 2):
            if any(org_word in ' '.join(words[i:i+3]) for org_word in ['phÃ²ng', 'ban', 'khoa', 'viá»‡n']):
                meaningful_phrases.append(' '.join(words[i:i+3]))
        
        return keywords + meaningful_phrases
    
    def hard_match_metadata(self, keywords: List[str], metadata_config: Dict) -> Optional[Dict[str, Any]]:
        """Hard matching vá»›i keywords tá»« metadata config"""
        filters = {}
        
        # Get keyword mappings from config
        query_keywords = metadata_config.get('query_keywords', {})
        
        # Check department keywords
        if 'departments' in query_keywords:
            for dept, dept_keywords in query_keywords['departments'].items():
                # Exact match hoáº·c partial match
                for keyword in keywords:
                    for dept_keyword in dept_keywords:
                        if (keyword == dept_keyword or 
                            keyword in dept_keyword or 
                            dept_keyword in keyword):
                            filters['department'] = dept
                            print(f"ðŸŽ¯ Hard match found: '{keyword}' â†’ department='{dept}'")
                            break
                    if 'department' in filters:
                        break
                if 'department' in filters:
                    break
        
        # Check education level keywords
        if 'education_levels' in query_keywords:
            for level, level_keywords in query_keywords['education_levels'].items():
                for keyword in keywords:
                    for level_keyword in level_keywords:
                        if (keyword == level_keyword or 
                            keyword in level_keyword or 
                            level_keyword in keyword):
                            filters['education_level'] = level
                            print(f"ðŸŽ¯ Hard match found: '{keyword}' â†’ education_level='{level}'")
                            break
                    if 'education_level' in filters:
                        break
                if 'education_level' in filters:
                    break
        
        return filters if filters else None
    
    def semantic_match_metadata(self, normalized_query: str, metadata_config: Dict) -> Optional[Dict[str, Any]]:
        """Semantic matching sá»­ dá»¥ng embeddings"""
        if not self.embedding_model:
            return None
        
        try:
            # Prepare department descriptions for semantic comparison
            departments = metadata_config.get('query_keywords', {}).get('departments', {})
            
            if not departments:
                return None
            
            # Create semantic descriptions for each department
            dept_descriptions = {}
            for dept_key, keywords in departments.items():
                # Táº¡o description tá»« keywords
                desc = ' '.join(keywords[:10])  # Láº¥y 10 keywords Ä‘áº§u
                dept_descriptions[dept_key] = desc
            
            # Get embeddings for query and department descriptions
            query_embedding = self.embedding_model.encode([normalized_query])
            dept_embeddings = self.embedding_model.encode(list(dept_descriptions.values()))
            
            # Calculate cosine similarity
            similarities = np.dot(query_embedding, dept_embeddings.T)[0]
            max_similarity_idx = np.argmax(similarities)
            max_similarity = similarities[max_similarity_idx]
            
            # Check if similarity is above threshold
            if max_similarity >= self.similarity_threshold:
                dept_key = list(dept_descriptions.keys())[max_similarity_idx]
                print(f"ðŸ§  Semantic match found: similarity={max_similarity:.3f} â†’ department='{dept_key}'")
                return {'department': dept_key}
            else:
                print(f"ðŸ§  Semantic similarity too low: max={max_similarity:.3f} < threshold={self.similarity_threshold}")
                return None
                
        except Exception as e:
            print(f"âš ï¸  Semantic matching error: {e}")
            return None
    
    def intelligent_filter(self, query: str, metadata_config: Dict) -> Tuple[Optional[Dict[str, Any]], str]:
        """
        Intelligent metadata filtering vá»›i multi-level approach
        
        Returns:
            Tuple[Optional[Dict], str]: (metadata_filter, strategy_used)
        """
        # Step 1: Normalize query
        normalized_query = self.normalize_query(query)
        if not normalized_query:
            return None, "empty_query"
        
        print(f"ðŸ“ Normalized query: '{query}' â†’ '{normalized_query}'")
        
        # Step 2: Extract keywords
        keywords = self.extract_keywords_from_query(normalized_query)
        if not keywords:
            return None, "no_keywords"
        
        print(f"ðŸ”‘ Extracted keywords: {keywords}")
        
        # Step 3: Hard matching first
        hard_match_result = self.hard_match_metadata(keywords, metadata_config)
        if hard_match_result:
            return hard_match_result, "hard_match"
        
        print("âŒ Hard match failed, trying semantic matching...")
        
        # Step 4: Semantic matching
        semantic_match_result = self.semantic_match_metadata(normalized_query, metadata_config)
        if semantic_match_result:
            return semantic_match_result, "semantic_match"
        
        print("âŒ Semantic match failed, will fallback to hybrid retrieval")
        
        # Step 5: No filtering (fallback to hybrid retrieval)
        return None, "fallback"
    
    def filter_documents(self, documents: List[Document], metadata_filter: Dict[str, Any]) -> List[Document]:
        """Filter documents based on metadata"""
        if not metadata_filter:
            return documents
        
        filtered_docs = []
        for doc in documents:
            match = True
            for key, value in metadata_filter.items():
                if key not in doc.metadata or doc.metadata[key] != value:
                    match = False
                    break
            if match:
                filtered_docs.append(doc)
        
        return filtered_docs


# Integration vá»›i smart_retrieve function
def enhanced_smart_retrieve(retriever, query: str, metadata_config: Dict, 
                          similarity_threshold: float = 0.7,
                          min_results_threshold: int = 3) -> List[Document]:
    """
    Enhanced smart retrieve vá»›i intelligent metadata filtering
    """
    # Initialize intelligent filter
    intelligent_filter = IntelligentMetadataFilter(similarity_threshold)
    
    # Get intelligent metadata filter
    metadata_filter, strategy = intelligent_filter.intelligent_filter(query, metadata_config)
    
    print(f"ðŸŽ¯ Filtering strategy: {strategy}")
    
    if metadata_filter:
        print(f"ðŸ” Applied metadata filter: {metadata_filter}")
        
        # Try retrieval with filter
        filtered_results = retriever._get_relevant_documents(query, metadata_filter)
        
        # Check if we have enough results
        if len(filtered_results) >= min_results_threshold:
            print(f"âœ… Found {len(filtered_results)} results with {strategy}")
            return apply_context_boosting(filtered_results, query)
        else:
            print(f"âš ï¸  Only {len(filtered_results)} results with {strategy}, falling back to hybrid retrieval")
    
    # Fallback to hybrid retrieval (no filtering)
    print("ðŸ”„ Using hybrid retrieval (no metadata filtering)")
    initial_results = retriever._get_relevant_documents(query)
    return apply_context_boosting(initial_results, query)


# Placeholder for apply_context_boosting (import from your existing code)
def apply_context_boosting(documents: List[Document], query: str) -> List[Document]:
    """Placeholder - use your existing apply_context_boosting function"""
    return documents