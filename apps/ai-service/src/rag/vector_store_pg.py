"""
PgVectorStore - L∆∞u tr·ªØ v√† t√¨m ki·∫øm vector embeddings trong PostgreSQL v·ªõi pgvector.
Port t·ª´ NestJS ai.service.ts vector search sang Python.

Features:
- L∆∞u document embeddings v√†o PostgreSQL v·ªõi pgvector extension
- T√¨m ki·∫øm similar documents b·∫±ng cosine similarity
- T√≠ch h·ª£p v·ªõi GeminiClient ƒë·ªÉ t·∫°o embeddings
"""

import os
import asyncio
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import asyncpg
from dotenv import load_dotenv

load_dotenv()


@dataclass
class DocumentChunk:
    """ƒê·∫°i di·ªán cho m·ªôt chunk c·ªßa document."""
    id: str
    user_storage_id: str
    content: str
    page_range: str
    title: Optional[str] = None
    similarity_score: Optional[float] = None
    created_at: Optional[datetime] = None


class PgVectorStore:
    """
    Vector Store s·ª≠ d·ª•ng PostgreSQL v·ªõi pgvector extension.

    C·∫•u h√¨nh:
    - DATABASE_URL: Connection string t·ªõi PostgreSQL
    """

    VECTOR_SEARCH_CONFIG = {
        "TOP_K": 15,
        "SIMILARITY_THRESHOLD": 0.7,
        "MAX_KEYWORDS": 10,
        "EMBEDDING_MODEL": "models/embedding-001",
    }

    def __init__(self, connection_string: Optional[str] = None):
        self.connection_string = connection_string or os.getenv("DATABASE_URL")
        self._pool: Optional[asyncpg.Pool] = None

    async def _get_pool(self) -> asyncpg.Pool:
        """L·∫•y ho·∫∑c t·∫°o connection pool."""
        if self._pool is None:
            self._pool = await asyncpg.create_pool(
                self.connection_string,
                min_size=2,
                max_size=10
            )
        return self._pool

    async def close(self):
        """ƒê√≥ng connection pool."""
        if self._pool:
            await self._pool.close()
            self._pool = None

    async def create_embedding(self, text: str, task_type: str = "retrieval_document") -> List[float]:
        """
        T·∫°o embedding vector cho text.
        S·ª≠ d·ª•ng GeminiClient ƒë·ªÉ c√≥ token rotation.

        Args:
            text: Text c·∫ßn embedding
            task_type: "retrieval_document" cho documents, "retrieval_query" cho queries
        """
        try:
            from llm.gemini_client import gemini_client
        except ImportError:
            from ..llm.gemini_client import gemini_client
        return await gemini_client.create_embedding(text, task_type)

    async def create_embeddings_batch(
        self,
        texts: List[str],
        task_type: str = "retrieval_document"
    ) -> List[List[float]]:
        """
        T·∫°o embeddings cho nhi·ªÅu texts v·ªõi batching.
        S·ª≠ d·ª•ng GeminiClient ƒë·ªÉ c√≥ token rotation v√† tr√°nh rate limit.
        """
        try:
            from llm.gemini_client import gemini_client
        except ImportError:
            from ..llm.gemini_client import gemini_client
        return await gemini_client.create_embeddings_batch(texts, task_type)

    async def store_document(
        self,
        doc_id: str,
        user_storage_id: str,
        content: str,
        page_range: str,
        title: Optional[str] = None
    ) -> bool:
        """
        L∆∞u document chunk v·ªõi embedding v√†o database.

        Args:
            doc_id: ID c·ªßa document chunk
            user_storage_id: ID c·ªßa file g·ªëc (UserStorage)
            content: N·ªôi dung text
            page_range: Ph·∫°m vi trang (vd: "1-3")
            title: Ti√™u ƒë·ªÅ (optional)

        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        try:
            # T·∫°o embedding cho content
            embedding = await self.create_embedding(content)

            pool = await self._get_pool()

            # Insert with ON CONFLICT UPDATE
            await pool.execute(
                """
                INSERT INTO "Document" (id, "userStorageId", content, "pageRange", title, embeddings, "createdAt", "updatedAt")
                VALUES ($1, $2, $3, $4, $5, $6::vector, NOW(), NOW())
                ON CONFLICT (id) DO UPDATE SET
                    content = EXCLUDED.content,
                    embeddings = EXCLUDED.embeddings,
                    "updatedAt" = NOW()
                """,
                doc_id,
                user_storage_id,
                content,
                page_range,
                title,
                f"[{','.join(str(v) for v in embedding)}]"
            )

            print(f"‚úÖ Stored document {doc_id} with {len(embedding)}-dim embedding")
            return True

        except Exception as e:
            print(f"‚ùå Error storing document: {e}")
            return False

    async def store_documents_batch(
        self,
        documents: List[Dict[str, Any]]
    ) -> int:
        """
        L∆∞u nhi·ªÅu document chunks v·ªõi batch embedding.
        S·ª≠ d·ª•ng batch embedding ƒë·ªÉ t·ªëi ∆∞u rate limit.

        Args:
            documents: List dict v·ªõi keys: id, user_storage_id, content, page_range, title (optional)

        Returns:
            S·ªë documents ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng
        """
        if not documents:
            return 0

        try:
            # T·∫°o batch embeddings
            contents = [doc['content'] for doc in documents]
            embeddings = await self.create_embeddings_batch(contents, "retrieval_document")

            pool = await self._get_pool()
            success_count = 0

            # Insert t·ª´ng document v·ªõi embedding t∆∞∆°ng ·ª©ng
            for i, doc in enumerate(documents):
                try:
                    await pool.execute(
                        """
                        INSERT INTO "Document" (id, "userStorageId", content, "pageRange", title, embeddings, "createdAt", "updatedAt")
                        VALUES ($1, $2, $3, $4, $5, $6::vector, NOW(), NOW())
                        ON CONFLICT (id) DO UPDATE SET
                            content = EXCLUDED.content,
                            embeddings = EXCLUDED.embeddings,
                            "updatedAt" = NOW()
                        """,
                        doc['id'],
                        doc['user_storage_id'],
                        doc['content'],
                        doc['page_range'],
                        doc.get('title'),
                        f"[{','.join(str(v) for v in embeddings[i])}]"
                    )
                    success_count += 1
                except Exception as e:
                    print(f"‚ùå Error storing document {doc['id']}: {e}")

            print(f"‚úÖ Stored {success_count}/{len(documents)} documents with batch embedding")
            return success_count

        except Exception as e:
            print(f"‚ùå Error in batch store: {e}")
            # Fallback: store t·ª´ng document m·ªôt
            print("‚ö†Ô∏è Falling back to individual document storage...")
            success_count = 0
            for doc in documents:
                success = await self.store_document(
                    doc['id'],
                    doc['user_storage_id'],
                    doc['content'],
                    doc['page_range'],
                    doc.get('title')
                )
                if success:
                    success_count += 1
            return success_count

    async def search_similar(
        self,
        user_storage_ids: List[str],
        query: str,
        top_k: int = None,
        similarity_threshold: float = None
    ) -> List[DocumentChunk]:
        """
        T√¨m ki·∫øm documents t∆∞∆°ng t·ª± b·∫±ng vector similarity.

        Args:
            user_storage_ids: Danh s√°ch file IDs ƒë·ªÉ search trong
            query: Query text
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ (default: 15)
            similarity_threshold: Ng∆∞·ª°ng similarity (default: 0.7)

        Returns:
            Danh s√°ch DocumentChunk s·∫Øp x·∫øp theo similarity gi·∫£m d·∫ßn
        """
        top_k = top_k or self.VECTOR_SEARCH_CONFIG["TOP_K"]
        similarity_threshold = similarity_threshold or self.VECTOR_SEARCH_CONFIG["SIMILARITY_THRESHOLD"]

        try:
            # T·∫°o embedding cho query (d√πng task_type kh√°c v·ªõi document)
            query_embedding = await self.create_embedding(query, task_type="retrieval_query")

            pool = await self._get_pool()

            # Query v·ªõi cosine similarity
            # 1 - (embedding <=> query_embedding) = cosine similarity
            rows = await pool.fetch(
                """
                SELECT
                    id, "userStorageId", "pageRange", title, content, "createdAt",
                    1 - (embeddings <=> $1::vector) as similarity_score
                FROM "Document"
                WHERE "userStorageId" = ANY($2::text[])
                  AND 1 - (embeddings <=> $1::vector) > $3
                ORDER BY embeddings <=> $1::vector ASC
                LIMIT $4
                """,
                f"[{','.join(str(v) for v in query_embedding)}]",
                user_storage_ids,
                similarity_threshold,
                top_k
            )

            results = [
                DocumentChunk(
                    id=row["id"],
                    user_storage_id=row["userStorageId"],
                    page_range=row["pageRange"],
                    title=row["title"],
                    content=row["content"],
                    similarity_score=row["similarity_score"],
                    created_at=row["createdAt"]
                )
                for row in rows
            ]

            print(f"üîç Found {len(results)} similar documents (threshold: {similarity_threshold})")
            return results

        except Exception as e:
            print(f"‚ùå Error in vector search: {e}")
            return []

    async def search_and_combine(
        self,
        user_storage_ids: List[str],
        query: str,
        top_k: int = 5,
        max_content_length: int = 6000
    ) -> Optional[str]:
        """
        T√¨m ki·∫øm t∆∞∆°ng t·ª± v√† k·∫øt h·ª£p content t·ª´ c√°c chunks.
        T∆∞∆°ng th√≠ch v·ªõi searchDocumentsByQuery trong NestJS.

        Args:
            user_storage_ids: Danh s√°ch file IDs
            query: Query text
            top_k: S·ªë chunks t·ªëi ƒëa
            max_content_length: ƒê·ªô d√†i content t·ªëi ƒëa

        Returns:
            Combined content string ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        similar_docs = await self.search_similar(
            user_storage_ids,
            query,
            top_k=top_k,
            similarity_threshold=0.5  # Lower threshold cho chat context
        )

        if not similar_docs:
            return None

        combined_content = ""
        for doc in similar_docs:
            chunk = f"[Trang {doc.page_range}]: {doc.content}\n\n"
            if len(combined_content) + len(chunk) > max_content_length:
                break
            combined_content += chunk

        return combined_content.strip() or None

    async def delete_by_user_storage(self, user_storage_id: str) -> int:
        """X√≥a t·∫•t c·∫£ documents c·ªßa m·ªôt UserStorage."""
        try:
            pool = await self._get_pool()
            result = await pool.execute(
                'DELETE FROM "Document" WHERE "userStorageId" = $1',
                user_storage_id
            )
            count = int(result.split()[-1])
            print(f"üóëÔ∏è Deleted {count} documents for userStorageId: {user_storage_id}")
            return count
        except Exception as e:
            print(f"‚ùå Error deleting documents: {e}")
            return 0


# Singleton instance
_pg_vector_store: Optional[PgVectorStore] = None


def get_pg_vector_store() -> PgVectorStore:
    """Get singleton PgVectorStore instance."""
    global _pg_vector_store
    if _pg_vector_store is None:
        _pg_vector_store = PgVectorStore()
    return _pg_vector_store
