"""
Semantic-based query analysis for metadata filtering
Replaces keyword matching with semantic similarity for better accuracy
"""

from typing import Dict, Any, List, Optional
import numpy as np
try:
    from langchain_ollama import OllamaEmbeddings
    OLLAMA_AVAILABLE = True
except ImportError:
    from sentence_transformers import SentenceTransformer
    OLLAMA_AVAILABLE = False
    print("‚ö†Ô∏è  OllamaEmbeddings not available, falling back to SentenceTransformer")
import json
import os
from pathlib import Path
from .metadata_config import get_metadata_config
from dotenv import load_dotenv
load_dotenv()
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
class SemanticQueryAnalyzer:
    def __init__(self):
        self.model = None
        self.model_type = None
        self.department_templates = None
        self.education_templates = None
        # Cache pre-computed embeddings
        self._dept_embeddings_cache = {}
        self._edu_embeddings_cache = {}
        self._cache_initialized = False
        self._initialize_templates()
    
    def _get_model(self):
        """Lazy loading of the embedding model - Ollama or SentenceTransformer"""
        if self.model is None:
            if OLLAMA_AVAILABLE:
                try:
                    # Use Ollama embeddings with nomic-embed-text model
                    self.model = OllamaEmbeddings(
                        model="nomic-embed-text",
                        base_url=OLLAMA_BASE_URL  # Default Ollama URL
                    )
                    self.model_type = "ollama"
                    print("üîó Using Ollama embeddings with nomic-embed-text model")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to initialize Ollama embeddings: {e}")
                    print("üîÑ Falling back to SentenceTransformer...")
                    self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    self.model_type = "sentence_transformer"
            else:
                # Fallback to SentenceTransformer
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                self.model_type = "sentence_transformer"
                print("üì¶ Using SentenceTransformer with multilingual model")
        
        return self.model
    
    def _encode_text(self, texts: List[str]) -> np.ndarray:
        """Encode texts using the appropriate model"""
        model = self._get_model()
        
        if self.model_type == "ollama":
            # Ollama embeddings return list of embeddings
            if isinstance(texts, str):
                texts = [texts]
            embeddings = model.embed_documents(texts)
            return np.array(embeddings)
        else:
            # SentenceTransformer 
            return model.encode(texts)
    
    def _initialize_templates(self):
        """Initialize semantic templates based on actual data folder content"""
        
        # First load static templates based on document analysis
        self._load_static_templates()
        
        # Then enhance with dynamic content from data folder
        self._enhance_templates_from_data_folder()
    
    def _load_static_templates(self):
        """Load static semantic templates based on document analysis"""
        
        # Department semantic templates - based on real document analysis
        self.department_templates = {
            'phongdaotao': [
                "quy ch·∫ø ƒë√†o t·∫°o ƒë·∫°i h·ªçc ch√≠nh quy theo h·ªá th·ªëng t√≠n ch·ªâ",
                "t·ªï ch·ª©c ƒë√†o t·∫°o v√† h·ªçc k·ª≥ ph·ª• trong nƒÉm h·ªçc",
                "ƒëƒÉng k√Ω kh·ªëi l∆∞·ª£ng h·ªçc t·∫≠p v√† k·∫ø ho·∫°ch h·ªçc t·∫≠p",
                "ki·ªÉm tra h·ªçc ph·∫ßn v√† ƒëi·ªÉm ki·ªÉm tra th∆∞·ªùng xuy√™n",
                "thi k·∫øt th√∫c h·ªçc ph·∫ßn v√† ƒëi·ªÉm thi k·∫øt th√∫c",
                "c√°ch t√≠nh ƒëi·ªÉm v√† thang ƒëi·ªÉm ƒë√°nh gi√°",
                "quy ƒë·ªãnh v·ªÅ c·ªë v·∫•n h·ªçc t·∫≠p v√† gi√°o vi√™n ch·ªß nhi·ªám",
                "x·∫øp h·∫°ng nƒÉm ƒë√†o t·∫°o v√† h·ªçc l·ª±c sinh vi√™n",
                "t·ª∑ l·ªá ph·∫ßn trƒÉm t√≠n ch·ªâ t√≠ch l≈©y theo t·ª´ng nƒÉm ƒë√†o t·∫°o",
                "sinh vi√™n nƒÉm th·ª© nh·∫•t th·ª© hai th·ª© ba th·ª© t∆∞ th·ª© nƒÉm",
                "s·ªë t√≠n ch·ªâ ph·∫£i t√≠ch l≈©y c·ªßa ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o",
                "20% 40% 60% 80% t·ªïng s·ªë t√≠n ch·ªâ to√†n kh√≥a h·ªçc",
                "ƒëi·ªÅu ki·ªán t·ªët nghi·ªáp v√† x√©t t·ªët nghi·ªáp",
                "ch∆∞∆°ng tr√¨nh gi√°o d·ª•c v√† m·ª•c ti√™u ƒë√†o t·∫°o c√°c b·∫≠c h·ªçc",
                "qu·∫£n l√Ω l·ªõp h·ªçc v√† l·ªõp h·ªçc ph·∫ßn",
                "quy ƒë·ªãnh th·ª±c hi·ªán v√† ch·∫•m ƒëi·ªÉm ƒë·ªÅ √°n t·ªët nghi·ªáp th·∫°c sƒ©"
            ],
            'phongkhaothi': [
                "quy ƒë·ªãnh v·ªÅ c√¥ng t√°c kh·∫£o th√≠ v√† t·ªï ch·ª©c thi c·ª≠",
                "ƒëi·ªÅu ki·ªán v√† s·ªë l·∫ßn d·ª± thi k·∫øt th√∫c h·ªçc ph·∫ßn",
                "ƒëi·ªÅu ki·ªán d·ª± thi k·∫øt th√∫c h·ªçc ph·∫ßn HVSV ƒë∆∞·ª£c ph√©p d·ª± thi",
                "ƒë√≥ng h·ªçc ph√≠ l·ªá ph√≠ ƒë·∫ßy ƒë·ªß ƒëi·ªÅu ki·ªán d·ª± thi",
                "ƒëi·ªÉm qu√° tr√¨nh ƒë·∫°t ng∆∞·ª°ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng",
                "tham gia t·ªëi thi·ªÉu 75% s·ªë gi·ªù h·ªçc tr√™n l·ªõp",
                "ch∆∞a ƒë∆∞·ª£c c√¥ng nh·∫≠n thi ƒë·∫°t v√† ch∆∞a s·ª≠ d·ª•ng h·∫øt s·ªë l·∫ßn d·ª± thi",
                "ki·ªÉm tra th∆∞·ªùng xuy√™n v√† thi gi·ªØa k·ª≥",
                "thi k·∫øt th√∫c h·ªçc ph·∫ßn v√† thi t·ªët nghi·ªáp", 
                "ƒë·ªì √°n lu·∫≠n vƒÉn ƒë·ªÅ √°n t·ªët nghi·ªáp",
                "h·ªôi ƒë·ªìng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng gi√°o d·ª•c",
                "ho·∫°t ƒë·ªông ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng gi√°o d·ª•c t·∫°i HVKTMM",
                "quy ƒë·ªãnh v·ªÅ ho·∫°t ƒë·ªông ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng gi√°o d·ª•c",
                "gi√°m s√°t v√† thanh tra c√¥ng t√°c thi c·ª≠",
                "ph√≤ng thi v√† quy tr√¨nh t·ªï ch·ª©c k·ª≥ thi",
                "x·ª≠ l√Ω vi ph·∫°m v√† k·ª∑ lu·∫≠t trong thi c·ª≠",
                "ƒë√°nh gi√° v√† ki·ªÉm ƒë·ªãnh ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o",
                "ng∆∞·ª°ng ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng v√† ƒëi·ªÅu ki·ªán ho√£n thi",
                "c√°n b·ªô coi thi v√† c√°n b·ªô ch·∫•m thi",
                "ph√∫c kh·∫£o b√†i thi v√† x·ª≠ l√Ω ch√™nh l·ªách ƒëi·ªÉm"
            ],
            'khoa': [
                "k·∫ø ho·∫°ch h·ªçc t·∫≠p ng√†nh v√† ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o",
                "ch∆∞∆°ng tr√¨nh gi√°o d·ª•c ƒë·∫°i h·ªçc c√°c ng√†nh h·ªçc",
                "m·ª•c ti√™u ƒë√†o t·∫°o v√† ch∆∞∆°ng tr√¨nh chuy√™n ng√†nh",
                "c√°c ng√†nh h·ªçc chuy√™n m√¥n k·ªπ thu·∫≠t v√† c√¥ng ngh·ªá",
                "chuy√™n ng√†nh an to√†n th√¥ng tin ATTT",
                "chuy√™n ng√†nh c√¥ng ngh·ªá th√¥ng tin CNTT", 
                "chuy√™n ng√†nh k·ªπ thu·∫≠t m·∫≠t m√£ v√† b·∫£o m·∫≠t",
                "chuy√™n ng√†nh ƒëi·ªán t·ª≠ vi·ªÖn th√¥ng",
                "ki·∫øn th·ª©c c∆° s·ªü ng√†nh v√† m√¥n h·ªçc chuy√™n ng√†nh",
                "c√°c khoa v√† ng√†nh h·ªçc chuy√™n m√¥n k·ªπ thu·∫≠t",
                "b·ªô m√¥n v√† ho·∫°t ƒë·ªông gi·∫£ng d·∫°y chuy√™n ng√†nh",
                "gi·∫£ng vi√™n b·ªô m√¥n v√† c√°n b·ªô gi·∫£ng d·∫°y",
                "nghi√™n c·ª©u khoa h·ªçc trong lƒ©nh v·ª±c chuy√™n m√¥n",
                "ph√°t tri·ªÉn ch∆∞∆°ng tr√¨nh gi√°o d·ª•c chuy√™n ng√†nh"
            ],
            'thongtinhvktmm': [
                "quy ch·∫ø ho·∫°t ƒë·ªông x√©t c√¥ng nh·∫≠n s√°ng ki·∫øn trong Ban C∆° y·∫øu",
                "ho·∫°t ƒë·ªông x√©t c√¥ng nh·∫≠n s√°ng ki·∫øn trong Ban C∆° y·∫øu Ch√≠nh ph·ªß",
                "phong tr√†o thi ƒëua Ban C∆° y·∫øu Ch√≠nh ph·ªß thi ƒëua ƒë·ªïi m·ªõi",
                "tri·ªÉn khai phong tr√†o b√¨nh d√¢n h·ªçc v·ª• s·ªë",
                "quy·∫øt ƒë·ªãnh ban h√†nh khung ki·∫øn th·ª©c k·ªπ nƒÉng s·ªë c∆° b·∫£n",
                "quy ch·∫ø qu·∫£n l√Ω s·ª≠ d·ª•ng h·ªá th·ªëng h·ªó tr·ª£ l√†m vi·ªác t·ª´ xa",
                "quy·∫øt ƒë·ªãnh th√†nh l·∫≠p t·ªï ·ª©ng d·ª•ng CNTT HVKTMM",
                "th√¥ng tin chung v·ªÅ H·ªçc vi·ªán K·ªπ thu·∫≠t m·∫≠t m√£",
                "Ban C∆° y·∫øu Ch√≠nh ph·ªß v√† c√°c ho·∫°t ƒë·ªông t·ªï ch·ª©c",
                "chuy·ªÉn ƒë·ªïi s·ªë v√† ·ª©ng d·ª•ng c√¥ng ngh·ªá th√¥ng tin",
                "s√°ng ki·∫øn c·∫£i ti·∫øn v√† ƒë·ªïi m·ªõi s√°ng t·∫°o"
            ],
            'viennghiencuuvahoptacphattrien': [
                "quy ch·∫ø ho·∫°t ƒë·ªông khoa h·ªçc v√† c√¥ng ngh·ªá KHCN",
                "th·ªëng nh·∫•t quy c√°ch vi·∫øt b√°o c√°o khoa h·ªçc",
                "qu·∫£n l√Ω ho·∫°t ƒë·ªông h·ª£p t√°c c·ªßa H·ªçc vi·ªán KTMM",
                "nhi·ªám v·ª• khoa h·ªçc v√† c√¥ng ngh·ªá c·∫•p tr√™n v√† c·∫•p c∆° s·ªü",
                "ho·∫°t ƒë·ªông nghi√™n c·ª©u khoa h·ªçc v√† ph√°t tri·ªÉn c√¥ng ngh·ªá",
                "h·ª£p t√°c qu·ªëc t·∫ø v√† chuy·ªÉn giao c√¥ng ngh·ªá",
                "qu·∫£n l√Ω d·ª± √°n v√† ƒë·ªÅ t√†i nghi√™n c·ª©u khoa h·ªçc",
                "xu·∫•t b·∫£n v√† c√¥ng b·ªë k·∫øt qu·∫£ nghi√™n c·ª©u",
                "ph√°t tri·ªÉn v√† ·ª©ng d·ª•ng c√¥ng ngh·ªá m·ªõi",
                "h·ª£p t√°c v·ªõi c√°c ƒë∆°n v·ªã trong v√† ngo√†i n∆∞·ªõc"
            ],
           
        }
        
        # Education level semantic templates - only for phongdaotao structure
        self.education_templates = {
            'daihoc': [
                "sinh vi√™n ƒë·∫°i h·ªçc v√† ch∆∞∆°ng tr√¨nh c·ª≠ nh√¢n",
                "b·∫≠c h·ªçc ƒë·∫°i h·ªçc v√† tr√¨nh ƒë·ªô bachelor", 
                "quy ch·∫ø ƒë√†o t·∫°o ƒë·∫°i h·ªçc ch√≠nh quy theo h·ªá th·ªëng t√≠n ch·ªâ",
                "ch∆∞∆°ng tr√¨nh gi√°o d·ª•c ƒë·∫°i h·ªçc h·ªá ch√≠nh quy",
                "m·ª•c ti√™u ƒë√†o t·∫°o ƒë·∫°i h·ªçc v√† ki·∫øn th·ª©c c·ª≠ nh√¢n",
                "k·∫ø ho·∫°ch h·ªçc t·∫≠p ng√†nh ƒë·∫°i h·ªçc",
                "ƒë√†o t·∫°o tr√¨nh ƒë·ªô ƒë·∫°i h·ªçc v√† sinh vi√™n"
            ],
            'thacsi': [
                "h·ªçc vi√™n th·∫°c sƒ© v√† ch∆∞∆°ng tr√¨nh master",
                "ƒë√†o t·∫°o tr√¨nh ƒë·ªô th·∫°c sƒ© v√† cao h·ªçc",
                "quy ƒë·ªãnh th·ª±c hi·ªán v√† ch·∫•m ƒëi·ªÉm ƒë·ªÅ √°n t·ªët nghi·ªáp th·∫°c sƒ©",
                "ch∆∞∆°ng tr√¨nh sau ƒë·∫°i h·ªçc b·∫≠c th·∫°c sƒ©",
                "nghi√™n c·ª©u n√¢ng cao v√† th·∫°c sƒ© khoa h·ªçc",
                "ƒë·ªÅ √°n t·ªët nghi·ªáp v√† lu·∫≠n vƒÉn th·∫°c sƒ©"
            ],
            'tiensi': [
                "nghi√™n c·ª©u sinh v√† ch∆∞∆°ng tr√¨nh ti·∫øn sƒ©",
                "ƒë√†o t·∫°o tr√¨nh ƒë·ªô ti·∫øn sƒ© v√† PhD",
                "nghi√™n c·ª©u khoa h·ªçc c·∫•p cao v√† lu·∫≠n √°n",
                "ch∆∞∆°ng tr√¨nh ti·∫øn sƒ© v√† nghi√™n c·ª©u sinh",
                "ho·∫°t ƒë·ªông nghi√™n c·ª©u khoa h·ªçc ti·∫øn sƒ©"
            ],
            'giangvien': [
                "gi·∫£ng vi√™n v√† ƒë·ªôi ng≈© gi·∫£ng d·∫°y",
                "c√°n b·ªô gi·∫£ng d·∫°y v√† gi√°o vi√™n",
                "ho·∫°t ƒë·ªông gi·∫£ng d·∫°y v√† gi√°o d·ª•c",
                "ph√°t tri·ªÉn ƒë·ªôi ng≈© gi·∫£ng vi√™n",
                "c·ªë v·∫•n h·ªçc t·∫≠p v√† gi√°o vi√™n ch·ªß nhi·ªám"
            ]
        }
    
    def _enhance_templates_from_data_folder(self):
        """Enhance templates with actual document names from data folder"""
        try:
            # Get the data folder path relative to this file
            current_dir = Path(__file__).parent.parent.parent
            data_dir = current_dir / "data"
            
            if not data_dir.exists():
                print(f"‚ö†Ô∏è  Data folder not found at {data_dir}")
                return
            
            # Process each department folder
            for dept_folder in data_dir.iterdir():
                if not dept_folder.is_dir():
                    continue
                    
                dept_name = dept_folder.name.lower()
                if dept_name not in self.department_templates:
                    continue
                
                # Extract document titles and add as templates
                doc_templates = self._extract_document_templates(dept_folder)
                if doc_templates:
                    # Add unique templates to existing ones
                    existing = set(self.department_templates[dept_name])
                    for template in doc_templates:
                        if template not in existing:
                            self.department_templates[dept_name].append(template)
                    
                    print(f"üìÅ Enhanced {dept_name} with {len(doc_templates)} document-based templates")
                
                # Only process education levels for phongdaotao
                if dept_name == 'phongdaotao':
                    education_templates = self._extract_education_templates(dept_folder)
                    if education_templates:
                        for edu_level, templates in education_templates.items():
                            if edu_level in self.education_templates:
                                existing = set(self.education_templates[edu_level])
                                for template in templates:
                                    if template not in existing:
                                        self.education_templates[edu_level].append(template)
                        print(f"üéì Enhanced education levels with templates from {dept_name}")
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Error enhancing templates from data folder: {e}")
    
    def _extract_document_templates(self, dept_folder: Path) -> List[str]:
        """Extract semantic templates from document names in a department folder"""
        templates = []
        
        def process_folder(folder: Path, level_prefix: str = ""):
            for item in folder.iterdir():
                if item.is_file() and item.suffix.lower() in ['.txt', '.docx', '.pdf']:
                    # Clean filename to create semantic template
                    filename = item.stem
                    # Remove numbering and clean up
                    clean_name = self._clean_document_name(filename)
                    if clean_name:
                        if level_prefix:
                            clean_name = f"{level_prefix} {clean_name}"
                        templates.append(clean_name.lower())
                elif item.is_dir():
                    # Recursively process subfolders (like daihoc, thacsi, etc.)
                    subfolder_prefix = item.name
                    process_folder(item, subfolder_prefix)
        
        process_folder(dept_folder)
        return list(set(templates))  # Remove duplicates
    
    def _extract_education_templates(self, phongdaotao_folder: Path) -> Dict[str, List[str]]:
        """Extract education level templates from phongdaotao subfolders"""
        education_templates = {}
        
        for item in phongdaotao_folder.iterdir():
            if item.is_dir():
                level_name = item.name.lower()
                if level_name in ['daihoc', 'thacsi', 'tiensi', 'giangvien']:
                    templates = []
                    for doc in item.iterdir():
                        if doc.is_file() and doc.suffix.lower() in ['.txt', '.docx', '.pdf']:
                            clean_name = self._clean_document_name(doc.stem)
                            if clean_name:
                                templates.append(f"{level_name} {clean_name}".lower())
                    
                    if templates:
                        education_templates[level_name] = templates
        
        return education_templates
    
    def _clean_document_name(self, filename: str) -> str:
        """Clean document filename to create meaningful semantic template"""
        # Remove leading numbers and dots
        import re
        cleaned = re.sub(r'^[\d\.]+\s*', '', filename)
        
        # Remove common prefixes
        cleaned = re.sub(r'^(Quy ƒë·ªãnh|Quy·∫øt ƒë·ªãnh|Ban h√†nh|Th√¥ng t∆∞)\s*', '', cleaned, flags=re.IGNORECASE)
        
        # Remove file extensions and version info
        cleaned = re.sub(r'\s*\([^)]*\)$', '', cleaned)  # Remove (version) at end
        cleaned = re.sub(r'\s*ver\s*\d+.*$', '', cleaned, flags=re.IGNORECASE)
        
        # Trim and return if meaningful
        cleaned = cleaned.strip()
        return cleaned if len(cleaned) > 10 else ""  # Only return if meaningful length
    
    def _initialize_embeddings_cache(self):
        """Pre-compute and cache all template embeddings"""
        if self._cache_initialized:
            return
        
        print("üöÄ Initializing embeddings cache...")
        
        # Cache department embeddings
        for dept, templates in self.department_templates.items():
            self._dept_embeddings_cache[dept] = self._encode_text(templates)
        
        # Cache education level embeddings  
        for level, templates in self.education_templates.items():
            self._edu_embeddings_cache[level] = self._encode_text(templates)
        
        self._cache_initialized = True
        print("‚úÖ Embeddings cache initialized successfully!")
    
    def analyze_query_semantic(self, query: str, confidence_threshold: float = 0.60) -> Dict[str, Any]:
        """
        Analyze query using semantic similarity instead of keyword matching
        
        Args:
            query: User query to analyze
            confidence_threshold: Minimum confidence score to apply filtering
            
        Returns:
            Dictionary with detected metadata filters and confidence scores
        """
        model = self._get_model()
        result = {
            'filters': {},
            'confidence_scores': {},
            'use_metadata_filtering': False,
            'reasoning': ''
        }
        
        # Check if query contains generic department terms that should search full DB
        generic_terms = ['ph√≤ng', 'ban', 'khoa', 'trung t√¢m', 'vi·ªán', 'nhi·ªám v·ª•', 'ch·ª©c nƒÉng']
        query_lower = query.lower()
        
        # If query mentions specific but unknown departments, use full search
        if any(term in query_lower for term in generic_terms):
            unknown_dept_indicators = ['thi·∫øt b·ªã', 'qu·∫£n tr·ªã', 'h√†nh ch√≠nh', 't√†i ch√≠nh', 'ch√≠nh tr·ªã']
            if any(indicator in query_lower for indicator in unknown_dept_indicators):
                result['reasoning'] = f"Generic department query detected - using full database search"
                return result
        
        # Ensure embeddings cache is initialized
        self._initialize_embeddings_cache()
        
        # Store query for education level analysis
        self._last_query = query
        
        # Encode the query ONCE
        query_embedding = self._encode_text([query])
        
        # Analyze department using cached embeddings
        dept_result = self._analyze_department_cached(query_embedding)
        if dept_result['confidence'] >= confidence_threshold:
            result['filters']['department'] = dept_result['department']
            result['confidence_scores']['department'] = dept_result['confidence']
            result['use_metadata_filtering'] = True
        
        # Analyze education level - only for phongdaotao department
        if dept_result['department'] == 'phongdaotao':
            level_result = self._analyze_education_level_cached(query_embedding)
            if level_result['confidence'] >= confidence_threshold:
                result['filters']['education_level'] = level_result['education_level']
                result['confidence_scores']['education_level'] = level_result['confidence']
                result['use_metadata_filtering'] = True
        
        # Generate reasoning
        reasoning_parts = []
        if 'department' in result['filters']:
            reasoning_parts.append(f"Department: {result['filters']['department']} (confidence: {result['confidence_scores']['department']:.2f})")
        if 'education_level' in result['filters']:
            reasoning_parts.append(f"Education Level: {result['filters']['education_level']} (confidence: {result['confidence_scores']['education_level']:.2f})")
        
        if result['use_metadata_filtering']:
            result['reasoning'] = f"Semantic analysis detected: {'; '.join(reasoning_parts)}"
        else:
            result['reasoning'] = f"Low confidence scores - using full database search (threshold: {confidence_threshold})"
        
        return result
    
    def _analyze_department_cached(self, query_embedding: np.ndarray) -> Dict[str, Any]:
        """Analyze department using cached embeddings"""
        best_dept = None
        best_score = 0.0
        
        for dept, cached_embeddings in self._dept_embeddings_cache.items():
            # Calculate similarity with cached embeddings
            similarities = np.dot(query_embedding, cached_embeddings.T).flatten()
            max_similarity = np.max(similarities)
            
            if max_similarity > best_score:
                best_score = max_similarity
                best_dept = dept
        
        return {
            'department': best_dept,
            'confidence': float(best_score)
        }
    
    def _analyze_education_level_cached(self, query_embedding: np.ndarray) -> Dict[str, Any]:
        """Analyze education level using cached embeddings"""
        # Store query for analysis
        if hasattr(self, '_last_query'):
            query_text = self._last_query.lower()
            
            # Skip education level detection for general queries about year classification
            if any(keyword in query_text for keyword in ['nƒÉm m·∫•y', 'nƒÉm th·ª©', 'ph·∫ßn trƒÉm', '%', 't·ª∑ l·ªá', 't√≠ch l≈©y']):
                return {
                    'education_level': None, 
                    'confidence': 0.0
                }
        
        best_level = None
        best_score = 0.0
        
        for level, cached_embeddings in self._edu_embeddings_cache.items():
            # Calculate similarity with cached embeddings
            similarities = np.dot(query_embedding, cached_embeddings.T).flatten()
            max_similarity = np.max(similarities)
            
            if max_similarity > best_score:
                best_score = max_similarity
                best_level = level
        
        return {
            'education_level': best_level,
            'confidence': float(best_score)
        }
    
    def _analyze_department(self, query_embedding: np.ndarray, model) -> Dict[str, Any]:
        """Analyze department using semantic similarity"""
        best_dept = None
        best_score = 0.0
        
        for dept, templates in self.department_templates.items():
            # Encode all templates for this department
            template_embeddings = self._encode_text(templates)
            
            # Calculate similarity with query
            similarities = np.dot(query_embedding, template_embeddings.T).flatten()
            
            # Use max similarity as the score for this department
            max_similarity = np.max(similarities)
            
            if max_similarity > best_score:
                best_score = max_similarity
                best_dept = dept
        
        return {
            'department': best_dept,
            'confidence': float(best_score)
        }
    
    def _analyze_education_level(self, query_embedding: np.ndarray, model) -> Dict[str, Any]:
        """Analyze education level using semantic similarity"""
        best_level = None
        best_score = 0.0
        
        for level, templates in self.education_templates.items():
            # Encode all templates for this level
            template_embeddings = self._encode_text(templates)
            
            # Calculate similarity with query
            similarities = np.dot(query_embedding, template_embeddings.T).flatten()
            
            # Use max similarity as the score for this level
            max_similarity = np.max(similarities)
            
            if max_similarity > best_score:
                best_score = max_similarity
                best_level = level
        
        return {
            'education_level': best_level,
            'confidence': float(best_score)
        }
    
    def get_department_mapping(self) -> Dict[str, str]:
        """Get Vietnamese names for departments"""
        config = get_metadata_config()
        mapping = {}
        
        folder_mappings = config.config.get('folder_mappings', {})
        for folder_name, folder_config in folder_mappings.items():
            dept_vn = folder_config.get('department_vn', '')
            if dept_vn:
                mapping[folder_config.get('department', folder_name)] = dept_vn
        
        return mapping

# Global instance for reuse
_semantic_analyzer = None

def get_semantic_analyzer() -> SemanticQueryAnalyzer:
    """Get singleton instance of semantic analyzer"""
    global _semantic_analyzer
    if _semantic_analyzer is None:
        _semantic_analyzer = SemanticQueryAnalyzer()
    return _semantic_analyzer

def analyze_query_semantic_filter(query: str, confidence_threshold: float = 0.60) -> Dict[str, Any]:
    """
    Main function to replace analyze_query_for_metadata_filter
    
    Args:
        query: User query to analyze
        confidence_threshold: Minimum confidence score to apply filtering
        
    Returns:
        Dictionary with metadata filters or empty dict for full search
    """
    analyzer = get_semantic_analyzer()
    result = analyzer.analyze_query_semantic(query, confidence_threshold)
    
    # Handle case where result might be None
    if result is None:
        print("‚ö†Ô∏è  Semantic analysis returned None - using full database search")
        return {}
    
    print(f"üîç Semantic Analysis: {result['reasoning']}")
    
    # Return only the filters for compatibility with existing code
    if result['use_metadata_filtering']:
        return result['filters']
    else:
        # Return empty dict to indicate no filtering (search full DB)
        return {}