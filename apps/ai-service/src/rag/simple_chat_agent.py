import logging
import os
from pathlib import Path
from typing import Dict, Any, List

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from llm import get_llm, LLMConfig
from rag.retriever import create_enhanced_hybrid_retriever, smart_retrieve, get_metadata_config, MetadataEnhancedHybridRetriever

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleChatAgent:
    """Simplified chat agent without LangGraph to avoid recursion issues"""

    def __init__(self, custom_retriever=None, model_name: str = None):
        """Initialize the Simple Chat Agent"""
        try:
            self.llm = get_llm()
            logger.info(f"Initialized LLM with runtime model selection")
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            raise

        # Store the retriever - allow None for general chat without documents
        if custom_retriever is not None:
            self.retriever = custom_retriever
        else:
            try:
                self.retriever = self.get_default_retriever()
            except Exception as e:
                logger.warning(f"No default retriever available: {e}. Will use LLM only for general chat.")
                self.retriever = None

        # Load prompts
        self.prompts = self._load_prompts()

    def get_default_retriever(self):
        """Get the default hybrid retriever for KMA regulations"""
        current_dir = Path(__file__).parent.absolute()
        project_root = current_dir.parent.parent
        vector_db_path = os.path.join(project_root, "vector_db")
        data_dir = os.path.join(project_root, "data")

        # Check if paths exist before creating retriever
        if not os.path.exists(vector_db_path) or not os.path.exists(data_dir):
            raise FileNotFoundError(f"Vector DB or data directory not found")

        # Use enhanced hybrid retriever with sliding window
        config = get_metadata_config()
        chunk_settings = config.get_chunk_settings()
        window_size = chunk_settings.get('sliding_window_size', 2)

        enhanced_retriever, _ = create_enhanced_hybrid_retriever(
            vector_db_path=vector_db_path,
            data_dir=data_dir,
            window_size=window_size
        )
        return enhanced_retriever

    def _load_prompts(self):
        """Load prompts from files"""
        prompts = {}
        prompts_dir = os.path.join(os.path.dirname(__file__), "prompts")

        try:
            with open(os.path.join(prompts_dir, "detailed_generate.txt"), "r", encoding="utf-8") as f:
                prompts["generate"] = f.read().strip()
                logger.info("Loaded detailed generate prompt")
        except FileNotFoundError:
            try:
                with open(os.path.join(prompts_dir, "generate.txt"), "r", encoding="utf-8") as f:
                    prompts["generate"] = f.read().strip()
                    logger.info("Loaded standard generate prompt")
            except FileNotFoundError:
                prompts["generate"] = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu.

Nhi·ªám v·ª•: H√£y ph√¢n t√≠ch k·ªπ th√¥ng tin ƒë∆∞·ª£c cung c·∫•p v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt, ƒë·∫ßy ƒë·ªß v√† h·ªØu √≠ch.

C√¢u h·ªèi: {question}

Th√¥ng tin t·ª´ t√†i li·ªáu:
{context}

Y√™u c·∫ßu tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi v·ªõi th√¥ng tin c·ª• th·ªÉ t·ª´ t√†i li·ªáu
2. Cung c·∫•p chi ti·∫øt v√† v√≠ d·ª• n·∫øu c√≥ trong t√†i li·ªáu
3. N·∫øu c√≥ nhi·ªÅu th√¥ng tin li√™n quan, h√£y t·ªï ch·ª©c th√†nh c√°c ƒëi·ªÉm r√µ r√†ng
4. N·∫øu th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√™u r√µ nh·ªØng g√¨ c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c
5. S·ª≠ d·ª•ng ng√¥n ng·ªØ r√µ r√†ng, d·ªÖ hi·ªÉu v√† t·ª± nhi√™n

Tr·∫£ l·ªùi chi ti·∫øt:"""
                logger.info("Using fallback detailed prompt")

        return prompts

    def check_relevance(self, query: str) -> bool:
        """Check if query requires document context"""
        if not self.retriever:
            return False

        prompt = f"""You are a smart assistant. The user has uploaded documents for context.
Query: "{query}"

Does this query require looking up information in the uploaded documents?
- Greeting (hello, hi, xin ch√†o): NO
- Identity questions (who are you, b·∫°n l√† ai): NO
- General knowledge questions (what is AI, 1+1=?): NO (unless explicitly asking 'according to the document')
- Specific questions about content (summarize this, what does it say about X, t√≥m t·∫Øt t√†i li·ªáu): YES

Answer only YES or NO."""

        try:
             response = self.llm.invoke([HumanMessage(content=prompt)])
             content = response.content.strip().upper()
             logger.info(f"Relevance check for '{query}': {content}")
             return "YES" in content
        except Exception as e:
            logger.error(f"Relevance check failed: {e}")
            return True

    def chat_stream(self, message: str, history: List[Dict[str, str]] = None):
        """Process a chat message and yield response chunks with history, dynamic RAG, and retry rotation"""
        from llm.gemini_client import GeminiClient
        import time

        def stream_with_retry(messages):
            max_retries = 15
            for attempt in range(max_retries):
                try:
                    for chunk in self.llm.stream(messages):
                        yield chunk.content
                    return
                except Exception as e:
                    error_msg = str(e)
                    is_quota = "429" in error_msg or "quota" in error_msg.lower() or "ResourceExhausted" in error_msg

                    if is_quota and attempt < max_retries - 1:
                        logger.warning(f"Quota exceeded (Attempt {attempt+1}/{max_retries}), rotating model/key...")
                        try:
                            client = GeminiClient()
                            current_model = getattr(self.llm, 'model', None) or getattr(self.llm, 'model_name', "gemini-2.0-flash")

                            should_rotate_key = client.mark_model_failed(current_model)

                            if should_rotate_key:
                                if hasattr(self.llm, 'google_api_key'):
                                    key = self.llm.google_api_key.get_secret_value()
                                    client.mark_key_failed(key)

                            self.llm = get_llm()
                            time.sleep(1)
                            continue
                        except Exception as rot_err:
                            logger.error(f"Rotation error: {rot_err}")
                            pass

                    if attempt == max_retries - 1 or not is_quota:
                        logger.error(f"Streaming failed: {e}")
                        yield f"Error: {e}"
                        return

        try:
            logger.info(f"Processing streaming query: {message}")
            if history is None:
                history = []

            # 1. Convert history
            lc_messages = []
            lc_messages.append(SystemMessage(content="B·∫°n l√† Sensei, m·ªôt tr·ª£ l√Ω AI th√¥ng minh h·ªó tr·ª£ h·ªçc t·∫≠p. H√£y tr·∫£ l·ªùi th√¢n thi·ªán, ch√≠nh x√°c v√† h·ªØu √≠ch."))

            for msg in history:
                role = msg.get('role')
                content = msg.get('content')
                if role == 'user':
                    lc_messages.append(HumanMessage(content=content))
                elif role == 'assistant':
                    lc_messages.append(AIMessage(content=content))

            # 2. Check relevance
            use_rag = False
            if self.retriever:
                use_rag = self.check_relevance(message)

            # 3. NO RAG
            if not use_rag:
                logger.info("Query not relevant to docs (or no docs), using general chat")
                lc_messages.append(HumanMessage(content=message))
                yield from stream_with_retry(lc_messages)
                return

            # 4. RAG
            logger.info("Query relevant to docs, retrieving context...")
            if isinstance(self.retriever, MetadataEnhancedHybridRetriever):
                docs = smart_retrieve(self.retriever, message, use_smart_filtering=True)
            else:
                docs = self.retriever.get_relevant_documents(message)

            context_docs = docs[:8]
            context = "\n\n---\n\n".join([
                f"ƒêo·∫°n {i+1}:\n{doc.page_content}"
                for i, doc in enumerate(context_docs)
            ])

            if context.strip():
                enhanced_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.

üéØ C√¢u h·ªèi: {message}

üìö Th√¥ng tin t·ª´ t√†i li·ªáu:
{context}

üìù H∆∞·ªõng d·∫´n:
‚Ä¢ Tr·∫£ l·ªùi chi ti·∫øt v√† ch√≠nh x√°c d·ª±a tr√™n t√†i li·ªáu
‚Ä¢ N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a th√¥ng tin, h√£y n√≥i r√µ
‚Ä¢ D·∫´n ch·ª©ng t·ª´ c√°c ƒëo·∫°n (v√≠ d·ª•: theo ƒêo·∫°n 1...)
"""
            else:
                enhanced_prompt = f"""Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu cho c√¢u h·ªèi: "{message}".
T√¥i s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung: {message}"""

            lc_messages.append(HumanMessage(content=enhanced_prompt))

            yield from stream_with_retry(lc_messages)

            if context.strip() and len(context_docs) > 0:
                yield f"\n\nüìã *Th√¥ng tin t·ª´ {len(context_docs)} ƒëo·∫°n tr√≠ch d·∫´n.*"

        except Exception as e:
            logger.error(f"Error in chat streaming: {str(e)}")
            yield f"Error: {str(e)}"

    def chat(self, message: str) -> str:
        """Process a chat message and return detailed response"""
        try:
            logger.info(f"Processing query: {message}")

            # If no retriever (general chat without documents), use LLM directly
            if self.retriever is None:
                logger.info("No retriever - using LLM directly for general chat")
                response = self.llm.invoke([{"role": "user", "content": message}])
                return response.content

            # Retrieve relevant documents using smart retrieval with sliding window
            if isinstance(self.retriever, MetadataEnhancedHybridRetriever):
                docs = smart_retrieve(self.retriever, message, use_smart_filtering=True)
            else:
                docs = self.retriever.get_relevant_documents(message)

            # Use more context for detailed answers
            context_docs = docs[:8]  # Increase from 5 to 8 for more context
            context = "\n\n---\n\n".join([
                f"ƒêo·∫°n {i+1}:\n{doc.page_content}"
                for i, doc in enumerate(context_docs)
            ])

            # Enhanced prompt for detailed responses
            if context.strip():
                # Add context about the query type for better responses
                enhanced_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, h√£y ph√¢n t√≠ch k·ªπ c√¢u h·ªèi v√† th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.

üéØ C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi: {message}

üìö Th√¥ng tin t·ª´ t√†i li·ªáu (ƒë∆∞·ª£c chia th√†nh c√°c ƒëo·∫°n):
{context}

üìù H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
‚Ä¢ ƒê·ªçc k·ªπ t·∫•t c·∫£ c√°c ƒëo·∫°n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
‚Ä¢ T·ªïng h·ª£p v√† ph√¢n t√≠ch ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß nh·∫•t
‚Ä¢ S·∫Øp x·∫øp th√¥ng tin theo logic, chia th√†nh c√°c ph·∫ßn r√µ r√†ng
‚Ä¢ Cung c·∫•p v√≠ d·ª• c·ª• th·ªÉ t·ª´ t√†i li·ªáu n·∫øu c√≥
‚Ä¢ S·ª≠ d·ª•ng bullet points ho·∫∑c s·ªë th·ª© t·ª± khi ph√π h·ª£p
‚Ä¢ N·∫øu c√≥ nhi·ªÅu kh√≠a c·∫°nh, h√£y tr√¨nh b√†y t·ª´ng kh√≠a c·∫°nh m·ªôt c√°ch c√≥ h·ªá th·ªëng

üí¨ C√¢u tr·∫£ l·ªùi chi ti·∫øt:"""
            else:
                enhanced_prompt = f"""Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë√£ upload ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: "{message}"

Vui l√≤ng th·ª≠:
‚Ä¢ ƒê·∫∑t c√¢u h·ªèi kh√°c li√™n quan ƒë·∫øn n·ªôi dung t√†i li·ªáu
‚Ä¢ S·ª≠ d·ª•ng t·ª´ kh√≥a kh√°c c√≥ th·ªÉ c√≥ trong t√†i li·ªáu
‚Ä¢ Ki·ªÉm tra l·∫°i xem t√†i li·ªáu c√≥ ch·ª©a th√¥ng tin b·∫°n ƒëang t√¨m kh√¥ng

T√¥i s·∫Ω c·ªë g·∫Øng tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t: {message}"""

            response = self.llm.invoke([{"role": "user", "content": enhanced_prompt}])

            # Post-process response to add more details if needed
            answer = response.content

            # Add source information at the end
            if context.strip() and len(context_docs) > 0:
                answer += f"\n\nüìã *Th√¥ng tin ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ {len(context_docs)} ƒëo·∫°n li√™n quan trong t√†i li·ªáu.*"

            logger.info("Detailed response generated successfully")
            return answer

        except Exception as e:
            logger.error(f"Error in chat processing: {str(e)}")
            return f"‚ùå Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}\n\nVui l√≤ng th·ª≠ l·∫°i ho·∫∑c ƒë·∫∑t c√¢u h·ªèi kh√°c."


async def process_simple_query(query: str, retriever=None, llm=None) -> Dict[str, Any]:
    """Simple query processing function without LangGraph"""
    try:
        # Create agent
        agent = SimpleChatAgent(custom_retriever=retriever)

        # Process query
        answer = agent.chat(query)

        # Get sources using smart retrieval
        if isinstance(agent.retriever, MetadataEnhancedHybridRetriever):
            docs = smart_retrieve(agent.retriever, query, use_smart_filtering=True)
        else:
            docs = agent.retriever.get_relevant_documents(query)
        sources = [doc.page_content for doc in docs[:3]]

        return {
            "answer": answer,
            "sources": sources,
            "source_type": "simple_agent"
        }

    except Exception as e:
        logger.error(f"Error in simple query processing: {str(e)}")
        return {
            "answer": f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}",
            "sources": [],
            "source_type": "error"
        }
