"""
LLM Factory ƒë·ªÉ t·∫°o c√°c instance model kh√°c nhau d·ª±a tr√™n lo·∫°i model ƒëang ho·∫°t ƒë·ªông.
"""
from typing import Optional, Dict, Any, List
from langchain_core.language_models import BaseChatModel
from langchain.callbacks.manager import CallbackManager
from langchain_ollama import ChatOllama
from langchain_google_genai import ChatGoogleGenerativeAI
import os

from .HFChatModel import HuggingFaceChatModel
from .model_manager import model_manager, ModelType

class LLMFactory:
    """Factory ƒë·ªÉ t·∫°o c√°c instance LLM kh√°c nhau d·ª±a tr√™n c·∫•u h√¨nh."""

    @classmethod
    def create_llm(cls, callback_manager: Optional[CallbackManager] = None) -> BaseChatModel:
        """
        T·∫°o instance LLM d·ª±a tr√™n model ƒëang ho·∫°t ƒë·ªông v·ªõi fallback logic.

        Args:
            callback_manager: Optional callback manager cho tracing

        Returns:
            BaseChatModel: Instance LLM t∆∞∆°ng ·ª©ng
        """
        # L·∫•y lo·∫°i model ƒëang ho·∫°t ƒë·ªông
        model_type = model_manager.get_model_type()

        # L·∫•y c√°c tham s·ªë chung
        temperature = model_manager.get_temperature()
        max_tokens = model_manager.get_max_tokens()

        # T·∫°o instance model t∆∞∆°ng ·ª©ng v·ªõi fallback logic
        if model_type == ModelType.OLLAMA:
            try:
                print("ü§ñ Attempting to create Ollama model...")
                return cls._create_ollama_model(temperature, max_tokens, callback_manager)
            except Exception as e:
                print(f"‚ö†Ô∏è Ollama model failed: {e}")
                print("üîÑ Falling back to Gemini model...")
                return cls._create_gemini_model(temperature, max_tokens, callback_manager)
        elif model_type == ModelType.GEMINI:
            return cls._create_gemini_model(temperature, max_tokens, callback_manager)
        else:  # HUGGINGFACE ho·∫∑c lo·∫°i kh√°c
            return cls._create_huggingface_model(temperature, max_tokens, callback_manager)

    @classmethod
    def _create_ollama_model(cls, temperature: float, max_tokens: int,
                            callback_manager: Optional[CallbackManager] = None) -> ChatOllama:
        """T·∫°o model Ollama."""
        ollama_info = model_manager.get_ollama_info()

        print(f"üîß Creating ChatOllama with:")
        print(f"   - model: {ollama_info['model']}")
        print(f"   - url: {ollama_info['url']}")
        print(f"   - temperature: {temperature}")
        print(f"   - max_tokens: {max_tokens}")

        return ChatOllama(
            model=ollama_info["model"],
            base_url=ollama_info["url"],  # Try base_url instead of url
            temperature=temperature,
            num_predict=max_tokens,  # Try num_predict instead of max_tokens
            callback_manager=callback_manager
        )

    @classmethod
    def _create_gemini_model(cls, temperature: float, max_tokens: int,
                            callback_manager: Optional[CallbackManager] = None) -> ChatGoogleGenerativeAI:
        """T·∫°o model Gemini v·ªõi multi-key/model rotation."""
        from .gemini_client import gemini_client

        # S·ª≠ d·ª•ng GeminiClient ƒë·ªÉ l·∫•y key/model v·ªõi rotation
        return gemini_client.get_chat_model(temperature=temperature)

    @classmethod
    def _create_huggingface_model(cls, temperature: float, max_tokens: int,
                                callback_manager: Optional[CallbackManager] = None) -> HuggingFaceChatModel:
        """T·∫°o model Hugging Face."""
        hf_info = model_manager.get_huggingface_info()

        # ƒê·∫∑t HF_TOKEN
        os.environ["HF_TOKEN"] = hf_info["token"]

        return HuggingFaceChatModel(
            model_path=hf_info["model"],
            temperature=temperature,
            max_tokens=max_tokens
        )
