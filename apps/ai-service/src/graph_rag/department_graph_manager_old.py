"""
Department Graph Manager - ENHANCED WITH SEMANTIC SIMILARITY
Quáº£n lÃ½ graph riÃªng biá»‡t cho tá»«ng phÃ²ng ban/Ä‘Æ¡n vá»‹
Sá»­ dá»¥ng dual-signal approach vá»›i semantic similarity
"""
import os
import logging
from typing import Dict, List, Optional, Set, Any
import networkx as nx
from langchain_core.documents import Document

from .graph_builder import DocumentGraph
from .subgraph_partitioner import SubgraphPartitioner
from .graph_retriever import GraphRoutedRetriever
from .semantic_department_detector import SemanticDepartmentDetector, DepartmentDecision

logger = logging.getLogger(__name__)


class DepartmentGraphManager:
    """
    Enhanced Department Graph Manager vá»›i semantic similarity
    Sá»­ dá»¥ng dual-signal approach Ä‘á»ƒ detect department chÃ­nh xÃ¡c hÆ¡n
    """
    
    def __init__(self, base_output_dir: str = "department_graphs"):
        self.base_output_dir = base_output_dir
        self.department_graphs: Dict[str, DocumentGraph] = {}
        self.department_partitioners: Dict[str, SubgraphPartitioner] = {}
        self.department_retrievers: Dict[str, GraphRoutedRetriever] = {}
        
        # Initialize semantic detector
        self.semantic_detector = SemanticDepartmentDetector(
            embeddings_dir=os.path.join(base_output_dir, "embeddings")
        )
        
        # Mapping phÃ²ng ban tá»« Ä‘Æ°á»ng dáº«n
        self.department_mapping = {
            'phongdaotao': ['phongdaotao', 'dao_tao', 'daotao'],
            'phongkhaothi': ['phongkhaothi', 'khao_thi', 'khaothi', 'chat_luong'],
            'khoa': ['khoa'],
            'viennghiencuuvahoptacphattrien': ['viennghiencuu', 'nghien_cuu', 'hop_tac'],
            'thongtinhvktmm': ['thongtin', 'hvktmm', 'hoc_vien'],
            'common': ['giao_trinh', 'chung']  # TÃ i liá»‡u chung
        }
        
        # Tá»« khÃ³a Ä‘á»ƒ xÃ¡c Ä‘á»‹nh phÃ²ng ban tá»« query - IMPROVED KEYWORDS
        self.department_keywords = {
            'phongdaotao': [
                # Core education keywords
                'Ä‘Ã o táº¡o', 'há»c táº­p', 'sinh viÃªn', 'há»c viÃªn', 'giáº£ng viÃªn', 'khÃ³a há»c', 'chÆ°Æ¡ng trÃ¬nh',
                'Ä‘áº¡i há»c', 'tháº¡c sÄ©', 'tiáº¿n sÄ©', 'cá»­ nhÃ¢n', 'cao há»c', 'luáº­n vÄƒn', 'luáº­n Ã¡n',
                'k68', 'k69', 'k70', 'há»c phÃ­', 'tuyá»ƒn sinh', 'tá»‘t nghiá»‡p',
                # Academic scoring - SPECIFIC PHRASES
                'Ä‘iá»ƒm há»c pháº§n', 'cÃ¡ch tÃ­nh Ä‘iá»ƒm', 'tÃ­nh Ä‘iá»ƒm', 'cÃ´ng thá»©c Ä‘iá»ƒm',
                'Ä‘iá»ƒm trung bÃ¬nh', 'Ä‘iá»ƒm tÃ­ch lÅ©y', 'xáº¿p loáº¡i há»c táº­p',
                # Course management
                'há»c pháº§n', 'tÃ­n chá»‰', 'mÃ´n há»c', 'bÃ i táº­p', 'thá»i khÃ³a biá»ƒu',
                'lá»‹ch há»c', 'phÃ²ng há»c', 'giÃ¡o trÃ¬nh', 'Ä‘á» cÆ°Æ¡ng'
            ],
            'phongkhaothi': [
                # Core examination keywords  
                'kháº£o thÃ­', 'thi', 'kiá»ƒm tra', 'Ä‘Ã¡nh giÃ¡', 'cháº¥t lÆ°á»£ng',
                'quy Ä‘á»•i Ä‘iá»ƒm', 'toeic', 'ielts', 'toefl', 'cambridge', 'tiáº¿ng anh',
                'ká»³ thi', 'Ä‘á» thi', 'coi thi', 'cháº¥m thi', 
                # Regulations and rules - HIGH PRIORITY
                'quy Ä‘á»‹nh', 'cÃ´ng tÃ¡c kháº£o thÃ­', 'quy cháº¿ thi', 'ká»· luáº­t thi',
                'phÃ²ng thi', 'giÃ¡m thá»‹', 'thÃ­ sinh', 'bÃ i thi', 'Ä‘iá»ƒm thi',
                # Specific exam processes
                'thi káº¿t thÃºc', 'thi giá»¯a ká»³', 'thi phá»¥', 'phÃºc kháº£o',
                'miá»…n thi', 'hoÃ£n thi', 'thi láº¡i', 'coi thi', 'cháº¥m thi'
            ],
            'khoa': [
                'khoa', 'ngÃ nh', 'chuyÃªn ngÃ nh', 'attt', 'cntt', 'dtvt', 'an toÃ n thÃ´ng tin',
                'cÃ´ng nghá»‡ thÃ´ng tin', 'Ä‘iá»‡n tá»­ viá»…n thÃ´ng', 'bá»™ mÃ´n', 'giÃ¡o trÃ¬nh'
            ],
            'viennghiencuuvahoptacphattrien': [
                'nghiÃªn cá»©u', 'khoa há»c', 'há»£p tÃ¡c', 'phÃ¡t triá»ƒn', 'Ä‘á» tÃ i', 'dá»± Ã¡n',
                'cÃ´ng bá»‘', 'táº¡p chÃ­', 'há»™i tháº£o', 'bÃ¡o cÃ¡o', 'sÃ¡ng cháº¿'
            ],
            'thongtinhvktmm': [
                'há»c viá»‡n', 'hvktmm', 'cÆ¡ yáº¿u', 'chuyá»ƒn Ä‘á»•i sá»‘', 'sÃ¡ng kiáº¿n',
                'giá»›i thiá»‡u', 'lá»‹ch sá»­', 'tá»• chá»©c', 'ban giÃ¡m hiá»‡u'
            ]
        }
    
    def detect_department_from_path(self, file_path: str) -> str:
        """
        XÃ¡c Ä‘á»‹nh phÃ²ng ban tá»« Ä‘Æ°á»ng dáº«n file
        """
        file_path_lower = file_path.lower().replace('\\', '/').replace(' ', '_')
        
        for dept, variants in self.department_mapping.items():
            for variant in variants:
                if variant in file_path_lower:
                    return dept
        
        # Default: common (tÃ i liá»‡u chung)
        return 'common'
    
    def detect_department_smart(
        self, 
        query: str, 
        user_metadata: Dict[str, Any] = None,
        top_k: int = 2
    ) -> DepartmentDecision:
        """
        Smart department detection sá»­ dá»¥ng dual-signal approach
        """
        return self.semantic_detector.detect_department(query, user_metadata)
    
    def detect_department_from_query(self, query: str, top_k: int = 2) -> List[str]:
        """
        Legacy method - kept for backward compatibility
        Sá»­ dá»¥ng semantic detection nhÆ°ng tráº£ vá» format cÅ©
        """
        decision = self.semantic_detector.detect_department(query, user_metadata={'role': 'student'})
        
        # Náº¿u permission denied, fallback to common
        if not decision.permission_granted:
            logger.warning(f"ğŸš« Permission denied for department {decision.chosen_department}")
            return ['common']
        
        # Return chosen department + fallbacks
        result = [decision.chosen_department]
        
        # Add other high-confidence departments as fallbacks
        for signal in decision.signals:
            if (signal.department != decision.chosen_department and 
                signal.confidence > 0.3 and 
                signal.department not in result):
                result.append(signal.department)
        
        return result[:top_k]
        """
        XÃ¡c Ä‘á»‹nh phÃ²ng ban liÃªn quan tá»« query - IMPROVED VERSION
        Tráº£ vá» list cÃ¡c phÃ²ng ban cÃ³ thá»ƒ liÃªn quan (theo thá»© tá»± Æ°u tiÃªn)
        """
        query_lower = query.lower()
        department_scores = {}
        
        # High-priority phrase patterns
        phrase_patterns = {
            'phongkhaothi': [
                'cÃ´ng tÃ¡c kháº£o thÃ­', 'quy Ä‘á»‹nh kháº£o thÃ­', 'ká»· luáº­t thi', 
                'quy cháº¿ thi', 'cÃ´ng tÃ¡c thi', 'phÃ²ng thi'
            ],
            'phongdaotao': [
                'Ä‘iá»ƒm há»c pháº§n', 'cÃ¡ch tÃ­nh Ä‘iá»ƒm', 'Ä‘iá»ƒm trung bÃ¬nh',
                'chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o', 'káº¿ hoáº¡ch há»c táº­p'
            ]
        }
        
        # Score cho phrase matching (high priority)
        for dept, phrases in phrase_patterns.items():
            phrase_score = 0
            for phrase in phrases:
                if phrase in query_lower:
                    # Phrase matching cÃ³ Ä‘iá»ƒm cao
                    phrase_score += len(phrase.split()) * 3  # x3 multiplier for phrases
            
            if phrase_score > 0:
                department_scores[dept] = department_scores.get(dept, 0) + phrase_score
        
        # Score cho keyword matching
        for dept, keywords in self.department_keywords.items():
            keyword_score = 0
            matched_keywords = []
            
            for keyword in keywords:
                if keyword in query_lower:
                    # Tá»« khÃ³a dÃ i cÃ³ trá»ng sá»‘ cao hÆ¡n
                    weight = len(keyword.split())
                    
                    # Special weighting rules
                    if dept == 'phongdaotao' and 'Ä‘iá»ƒm' in keyword:
                        # Æ¯u tiÃªn "Ä‘iá»ƒm há»c pháº§n" hÆ¡n "Ä‘iá»ƒm" Ä‘Æ¡n láº»
                        if keyword == 'Ä‘iá»ƒm há»c pháº§n':
                            weight *= 2
                        elif keyword == 'Ä‘iá»ƒm':
                            weight *= 0.5  # Reduce weight for generic "Ä‘iá»ƒm"
                    
                    if dept == 'phongkhaothi' and keyword == 'quy Ä‘á»‹nh':
                        weight *= 1.5  # Boost "quy Ä‘á»‹nh" for kháº£o thÃ­
                    
                    keyword_score += weight
                    matched_keywords.append(keyword)
            
            if keyword_score > 0:
                department_scores[dept] = department_scores.get(dept, 0) + keyword_score
                logger.debug(f"ğŸ” {dept}: {matched_keywords} -> score: {keyword_score}")
        
        # Remove very low scores (likely false positives)
        department_scores = {dept: score for dept, score in department_scores.items() if score >= 1.0}
        
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm sá»‘
        sorted_depts = sorted(department_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Log scoring for debugging
        logger.debug(f"ğŸ¯ Query: '{query[:50]}...'")
        for dept, score in sorted_depts:
            logger.debug(f"   {dept}: {score}")
        
        # Tráº£ vá» top-k phÃ²ng ban
        result = [dept for dept, score in sorted_depts[:top_k]]
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y phÃ²ng ban cá»¥ thá»ƒ, tÃ¬m trong táº¥t cáº£
        if not result:
            logger.warning(f"KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c phÃ²ng ban tá»« query: {query[:100]}")
            return list(self.department_graphs.keys())
        
        return result
    
    def build_department_graphs(self, documents: List[Document]) -> Dict[str, int]:
        """
        XÃ¢y dá»±ng graph riÃªng cho tá»«ng phÃ²ng ban tá»« documents
        
        Returns:
            Dict[department, node_count] - Thá»‘ng kÃª sá»‘ node má»—i phÃ²ng ban
        """
        logger.info("=" * 80)
        logger.info("ğŸ¢ BUILDING DEPARTMENT-SPECIFIC GRAPHS")
        logger.info("=" * 80)
        
        # PhÃ¢n loáº¡i documents theo phÃ²ng ban
        dept_documents = {}
        for doc in documents:
            source_path = doc.metadata.get('source', '')
            dept = self.detect_department_from_path(source_path)
            
            if dept not in dept_documents:
                dept_documents[dept] = []
            dept_documents[dept].append(doc)
        
        # Thá»‘ng kÃª
        logger.info(f"ğŸ“Š Documents by department:")
        for dept, docs in dept_documents.items():
            logger.info(f"   {dept}: {len(docs)} documents")
        
        # XÃ¢y dá»±ng graph cho tá»«ng phÃ²ng ban
        stats = {}
        os.makedirs(self.base_output_dir, exist_ok=True)
        
        for dept, docs in dept_documents.items():
            if len(docs) == 0:
                logger.warning(f"âš ï¸  Skipping {dept}: No documents")
                continue
                
            logger.info(f"\nğŸ”¨ Building graph for department: {dept}")
            logger.info(f"   Documents: {len(docs)}")
            
            # Táº¡o graph builder cho phÃ²ng ban nÃ y
            graph_builder = DocumentGraph(
                semantic_threshold=0.7,
                max_semantic_edges_per_node=7
            )
            
            # XÃ¢y dá»±ng graph
            graph = graph_builder.build_graph(docs)
            self.department_graphs[dept] = graph_builder
            
            # Táº¡o partitioner
            partitioner = SubgraphPartitioner(graph)
            communities = partitioner.partition_by_community_detection(algorithm='louvain')
            self.department_partitioners[dept] = partitioner
            
            # Táº¡o retriever
            retriever = GraphRoutedRetriever(
                graph=graph,
                partitioner=partitioner,
                k=4,
                internal_k=8,
                hop_depth=2,
                expansion_factor=1.5
            )
            self.department_retrievers[dept] = retriever
            
            # LÆ°u graph
            dept_output_dir = os.path.join(self.base_output_dir, dept)
            os.makedirs(dept_output_dir, exist_ok=True)
            graph_path = os.path.join(dept_output_dir, "graph.pkl")
            graph_builder.save_graph(graph_path)
            
            # Thá»‘ng kÃª
            stats[dept] = {
                'nodes': graph.number_of_nodes(),
                'edges': graph.number_of_edges(),
                'communities': len(communities),
                'avg_degree': 2 * graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0
            }
            
            logger.info(f"   âœ… Graph built: {stats[dept]['nodes']} nodes, {stats[dept]['edges']} edges")
            logger.info(f"   ğŸ“ Saved to: {graph_path}")
        
        # Tá»•ng káº¿t
        logger.info("\n" + "=" * 80)
        logger.info("âœ… DEPARTMENT GRAPHS BUILD COMPLETE!")
        logger.info("=" * 80)
        
        total_nodes = sum(s['nodes'] for s in stats.values())
        total_edges = sum(s['edges'] for s in stats.values())
        
        logger.info(f"ğŸ“Š Total: {len(stats)} departments, {total_nodes} nodes, {total_edges} edges")
        for dept, stat in stats.items():
            logger.info(f"   {dept}: {stat['nodes']} nodes, {stat['communities']} communities")
        
        return {dept: stat['nodes'] for dept, stat in stats.items()}
    
    def load_department_graphs(self) -> bool:
        """
        Load cÃ¡c graph Ä‘Ã£ xÃ¢y dá»±ng tá»« disk
        
        Returns:
            bool - True náº¿u load thÃ nh cÃ´ng
        """
        if not os.path.exists(self.base_output_dir):
            logger.error(f"Department graphs directory not found: {self.base_output_dir}")
            return False
        
        loaded_count = 0
        for dept_name in os.listdir(self.base_output_dir):
            dept_dir = os.path.join(self.base_output_dir, dept_name)
            if not os.path.isdir(dept_dir):
                continue
                
            graph_path = os.path.join(dept_dir, "graph.pkl")
            if not os.path.exists(graph_path):
                logger.warning(f"Graph file not found for {dept_name}: {graph_path}")
                continue
            
            try:
                # Load graph
                graph_builder = DocumentGraph()
                graph_builder.load_graph(graph_path)
                self.department_graphs[dept_name] = graph_builder
                
                # Recreate partitioner
                partitioner = SubgraphPartitioner(graph_builder.graph)
                # Re-run community detection
                communities = partitioner.partition_by_community_detection(algorithm='louvain')
                self.department_partitioners[dept_name] = partitioner
                
                # Recreate retriever
                retriever = GraphRoutedRetriever(
                    graph=graph_builder.graph,
                    partitioner=partitioner,
                    k=4,
                    internal_k=8,
                    hop_depth=2,
                    expansion_factor=1.5
                )
                self.department_retrievers[dept_name] = retriever
                
                loaded_count += 1
                logger.info(f"âœ… Loaded graph for {dept_name}: {graph_builder.graph.number_of_nodes()} nodes")
                
            except Exception as e:
                logger.error(f"âŒ Failed to load graph for {dept_name}: {e}")
        
        logger.info(f"ğŸ“Š Loaded {loaded_count} department graphs")
        return loaded_count > 0
    
    def query_department(self, query: str, department: str, k: int = 4) -> List[Document]:
        """
        Query trong graph cá»§a phÃ²ng ban cá»¥ thá»ƒ
        
        Args:
            query: CÃ¢u há»i
            department: TÃªn phÃ²ng ban
            k: Sá»‘ lÆ°á»£ng documents tráº£ vá»
            
        Returns:
            List[Document] - Káº¿t quáº£ tÃ¬m kiáº¿m
        """
        if department not in self.department_retrievers:
            logger.error(f"Department {department} not found in retrievers")
            return []
        
        logger.info(f"ğŸ” Querying department '{department}' with query: {query[:100]}")
        
        retriever = self.department_retrievers[department]
        retriever.k = k  # Update k dynamically
        
        try:
            results = retriever.get_relevant_documents(query)
            logger.info(f"âœ… Found {len(results)} documents in {department}")
            return results
        except Exception as e:
            logger.error(f"âŒ Error querying {department}: {e}")
            return []
    
    def query_multi_department(self, query: str, departments: List[str], k: int = 4) -> List[Document]:
        """
        Query trong nhiá»u phÃ²ng ban vÃ  merge káº¿t quáº£
        
        Args:
            query: CÃ¢u há»i
            departments: Danh sÃ¡ch phÃ²ng ban
            k: Sá»‘ lÆ°á»£ng documents tá»•ng cá»™ng
            
        Returns:
            List[Document] - Káº¿t quáº£ merged vÃ  ranked
        """
        all_results = []
        k_per_dept = max(1, k // len(departments)) if departments else k
        
        logger.info(f"ğŸ” Multi-department query: {departments}, k={k_per_dept} per dept")
        
        for dept in departments:
            if dept in self.department_retrievers:
                dept_results = self.query_department(query, dept, k_per_dept)
                # Add department info to metadata
                for doc in dept_results:
                    doc.metadata['query_department'] = dept
                all_results.extend(dept_results)
        
        # Re-rank combined results vÃ  limit to k
        if len(all_results) > k:
            # Sort by relevance score if available
            all_results.sort(key=lambda d: d.metadata.get('combined_score', 0), reverse=True)
            all_results = all_results[:k]
        
        logger.info(f"âœ… Multi-department query returned {len(all_results)} documents")
        return all_results
    
    def query_smart(self, query: str, user_department: str = None, k: int = 4) -> List[Document]:
        """
        Smart query - Tá»± Ä‘á»™ng xÃ¡c Ä‘á»‹nh phÃ²ng ban vÃ  query
        
        Args:
            query: CÃ¢u há»i
            user_department: PhÃ²ng ban cá»§a user (náº¿u cÃ³)
            k: Sá»‘ lÆ°á»£ng documents
            
        Returns:
            List[Document] - Káº¿t quáº£ tÃ¬m kiáº¿m
        """
        logger.info(f"ğŸ§  Smart query: '{query[:100]}', user_dept='{user_department}'")
        
        # 1. XÃ¡c Ä‘á»‹nh phÃ²ng ban tá»« query
        query_departments = self.detect_department_from_query(query, top_k=2)
        
        # 2. Æ¯u tiÃªn phÃ²ng ban cá»§a user náº¿u cÃ³
        target_departments = []
        if user_department and user_department in self.department_retrievers:
            target_departments.append(user_department)
        
        # 3. ThÃªm phÃ²ng ban tá»« query (náº¿u chÆ°a cÃ³)
        for dept in query_departments:
            if dept not in target_departments:
                target_departments.append(dept)
        
        # 4. Fallback: tÃ¬m trong táº¥t cáº£ phÃ²ng ban náº¿u khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c
        if not target_departments:
            target_departments = list(self.department_retrievers.keys())
            logger.warning(f"Using fallback: search all departments {target_departments}")
        
        logger.info(f"ğŸ¯ Target departments: {target_departments}")
        
        # 5. Query
        if len(target_departments) == 1:
            return self.query_department(query, target_departments[0], k)
        else:
            return self.query_multi_department(query, target_departments, k)
    
    def get_department_stats(self) -> Dict[str, Dict]:
        """
        Láº¥y thá»‘ng kÃª cá»§a táº¥t cáº£ department graphs
        """
        stats = {}
        for dept, graph_builder in self.department_graphs.items():
            graph = graph_builder.graph
            partitioner = self.department_partitioners.get(dept)
            
            stats[dept] = {
                'nodes': graph.number_of_nodes(),
                'edges': graph.number_of_edges(),
                'communities': len(partitioner.communities) if partitioner else 0,
                'avg_degree': 2 * graph.number_of_edges() / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0
            }
        
        return stats
    
    def list_available_departments(self) -> List[str]:
        """Liá»‡t kÃª cÃ¡c phÃ²ng ban cÃ³ sáºµn"""
        return list(self.department_retrievers.keys())