"""
AI Processing API - OCR v√† Vector Search

API n√†y CH·ªà x·ª≠ l√Ω AI tasks:
- OCR files ƒë√£ upload b·ªüi NestJS
- Vector similarity search
- Query files v·ªõi context

KH√îNG x·ª≠ l√Ω: Upload file, t·∫°o UserStorage (NestJS l√†m)
"""
import logging
import os
import sys
import uuid
from typing import Dict, Any, List, Optional, Annotated
from datetime import datetime

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field, ConfigDict

# Add the parent directory to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.rag.retriever import extract_text_from_file, create_in_memory_retriever
from src.rag.simple_chat_agent import SimpleChatAgent
from src.rag.vector_store_pg import get_pg_vector_store
from src.backend.services.ocr_service import ocr_service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# Cache retrievers in memory for faster queries
_retriever_cache: Dict[str, Any] = {}


# ==================== Request Models ====================

class ProcessFileRequest(BaseModel):
    """Request t·ª´ NestJS ƒë·ªÉ OCR file ƒë√£ upload"""
    model_config = ConfigDict(populate_by_name=True)
    
    user_storage_id: str = Field(..., description="ID c·ªßa UserStorage (NestJS ƒë√£ t·∫°o)")
    model_type: str = Field(default="fayedark", alias="modelType", description="AI model: 'gemini' or 'fayedark' (Ollama)")


class QueryFileRequest(BaseModel):
    """Request ƒë·ªÉ query file"""
    model_config = ConfigDict(populate_by_name=True)
    
    user_storage_id: str = Field(..., description="ID c·ªßa UserStorage")
    query: str = Field(..., description="C√¢u h·ªèi v·ªÅ n·ªôi dung file")
    model_type: Annotated[Optional[str], Field(default=None, alias="modelType", description="AI model type")] = None


class MultiQueryRequest(BaseModel):
    """Request ƒë·ªÉ query nhi·ªÅu c√¢u h·ªèi"""
    model_config = ConfigDict(populate_by_name=True)
    
    user_storage_id: str = Field(..., description="ID c·ªßa UserStorage")
    queries: List[str] = Field(..., description="Danh s√°ch c√¢u h·ªèi")
    model_type: Annotated[Optional[str], Field(default=None, alias="modelType", description="AI model type")] = None


# ==================== Helper Functions ====================

async def get_or_create_retriever(user_storage_id: str, model_type: str = "gemini"):
    """Get retriever from cache or create from DB chunks"""
    cache_key = f"{user_storage_id}:{model_type}"
    if cache_key in _retriever_cache:
        return _retriever_cache[cache_key]

    chunks = await ocr_service.get_document_chunks(user_storage_id)
    if not chunks:
        return None

    combined_content = "\n\n".join([c.content for c in chunks])
    retriever, _ = create_in_memory_retriever(combined_content, model_type=model_type)
    _retriever_cache[cache_key] = retriever
    return retriever


# ==================== API Endpoints ====================

@router.post("/process-file", response_model=Dict[str, Any])
async def process_file(request: ProcessFileRequest):
    """
    OCR v√† t·∫°o embeddings cho file ƒë√£ upload b·ªüi NestJS
    
    Flow:
    1. NestJS upload file l√™n R2, t·∫°o UserStorage v·ªõi status=PENDING
    2. NestJS g·ªçi API n√†y v·ªõi userStorageId
    3. Python download t·ª´ R2
    4. X·ª≠ l√Ω OCR v√† tr√≠ch xu·∫•t text (Fallback sang Tesseract cho b·∫£n scan)
    5. T·∫°o embedding cho m·ªói chunk v√† l∆∞u v√†o Document table
    6. Update status = COMPLETED
    """
    try:
        user_storage_id = request.user_storage_id
        model_type = request.model_type
        logger.info(f"Processing file: {user_storage_id} with model: {model_type}")

        # Get file info from DB (created by NestJS)
        file_info = await ocr_service.get_file_info(user_storage_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found in UserStorage")

        # Check if already processed
        if file_info.processing_status == "COMPLETED":
            chunks = await ocr_service.get_document_chunks(user_storage_id)
            logger.info(f"File already processed: {user_storage_id}, {len(chunks)} chunks")
            return {
                "success": True,
                "cached": True,
                "message": "File ƒë√£ ƒë∆∞·ª£c OCR tr∆∞·ªõc ƒë√≥",
                "chunks_count": len(chunks),
                "user_storage_id": user_storage_id
            }

        # Update status to PROCESSING
        await ocr_service.update_processing_status(user_storage_id, "PROCESSING")

        try:
            # Download file from R2
            logger.info(f"Downloading file from R2: {file_info.url}")
            file_bytes, temp_path = await ocr_service.download_file_from_r2(file_info.url)

            # --- LOCAL PROCESSING WITH TESSERACT FALLBACK ---
            from backend.services.pdf_ocr_service import pdf_ocr_service
            stored_count = 0
            pg_store = get_pg_vector_store()

            # Check file type
            is_pdf = file_info.mimetype == "application/pdf" or file_info.url.lower().endswith('.pdf')

            if is_pdf:
                logger.info("üìÑ Processing PDF with local Tesseract/PyPDF...")
                try:
                    # Process PDF with chunks (split by pages + OCR each chunk)
                    chunk_results = pdf_ocr_service.process_pdf_with_chunks(file_bytes)

                    # PDF chunks are by pages (10 pages/chunk), need to split text further
                    from langchain_text_splitters import RecursiveCharacterTextSplitter
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1500,  # Max 1500 chars per embedding chunk
                        chunk_overlap=150,
                        separators=["\n\n", "\n", ". ", " ", ""]
                    )

                    # Prepare documents for batch storage
                    documents_to_store = []
                    chunk_idx = 0
                    
                    for page_chunk_index, page_chunk_text in chunk_results:
                        if not page_chunk_text or not page_chunk_text.strip():
                            continue

                        # Split large page chunks into smaller text chunks
                        text_chunks = text_splitter.split_text(page_chunk_text)
                        logger.info(f"üìù Page chunk {page_chunk_index}: {len(page_chunk_text)} chars ‚Üí {len(text_chunks)} text chunks")
                        
                        for text_chunk in text_chunks:
                            if not text_chunk.strip():
                                continue
                            chunk_id = f"{user_storage_id}_chunk_{chunk_idx}"
                            documents_to_store.append({
                                'id': chunk_id,
                                'user_storage_id': user_storage_id,
                                'content': text_chunk.strip(),
                                'page_range': str(page_chunk_index),
                                'title': f"Chunk {chunk_idx + 1}"
                            })
                            chunk_idx += 1

                    if documents_to_store:
                        logger.info(f"üì¶ Storing {len(documents_to_store)} text chunks...")
                        stored_count = await pg_store.store_documents_batch(documents_to_store, model_type=model_type)
                except Exception as pdf_error:
                    logger.error(f"PDF processing failed: {pdf_error}")
                    raise pdf_error
            else:
                # Non-PDF: Extract text and chunk
                logger.info(f"üìù Processing non-PDF file: {file_info.mimetype}")
                from src.rag.retriever import extract_text_from_file
                file_content = extract_text_from_file(temp_path, file_info.mimetype)

                if file_content.startswith("Error") or file_content.startswith("Unsupported"):
                    raise HTTPException(status_code=400, detail=file_content)

                # Split into chunks
                from langchain_text_splitters import RecursiveCharacterTextSplitter
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
                text_chunks = text_splitter.split_text(file_content)

                # Prepare for batch storage
                documents_to_store = []
                for i, chunk_text in enumerate(text_chunks):
                    chunk_id = f"{user_storage_id}_chunk_{i}"
                    documents_to_store.append({
                        'id': chunk_id,
                        'user_storage_id': user_storage_id,
                        'content': chunk_text,
                        'page_range': str(i + 1),
                        'title': f"Chunk {i + 1}"
                    })

                if documents_to_store:
                    stored_count = await pg_store.store_documents_batch(documents_to_store, model_type=model_type)

            # Cleanup temp file
            if os.path.exists(temp_path):
                os.unlink(temp_path)

            if stored_count == 0:
                await ocr_service.update_processing_status(user_storage_id, "FAILED")
                raise HTTPException(status_code=400, detail="No content could be extracted from file")

            # Update status to COMPLETED
            await ocr_service.update_processing_status(user_storage_id, "COMPLETED", credit_charged=True)

            logger.info(f"‚úÖ File processed successfully: {user_storage_id}, {stored_count} chunks")

            return {
                "success": True,
                "cached": False,
                "message": "OCR v√† l∆∞u embeddings th√†nh c√¥ng (Tesseract Fallback)",
                "chunks_count": stored_count,
                "user_storage_id": user_storage_id
            }

        except Exception as e:
            await ocr_service.update_processing_status(user_storage_id, "FAILED")
            raise e
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")


@router.post("/query-file", response_model=Dict[str, Any])
async def query_file(request: QueryFileRequest):
    """Query file content using vector search"""
    try:
        user_storage_id = request.user_storage_id
        query = request.query

        logger.info(f"Querying file: {user_storage_id}")

        # Check file exists and processed
        file_info = await ocr_service.get_file_info(user_storage_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")

        if file_info.processing_status != "COMPLETED":
            raise HTTPException(status_code=400, detail=f"File ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω: {file_info.processing_status}")

        # Determine model_type
        from src.llm.model_manager import model_manager, ModelType
        system_model_type = model_manager.get_model_type()
        requested_model = request.model_type
        
        # Consistent mapping: translate system ModelType to string id
        if not requested_model:
            requested_model = "fayedark" if system_model_type == ModelType.OLLAMA else "gemini"

        # Vector search
        pg_store = get_pg_vector_store()
        similar_docs = await pg_store.search_similar([user_storage_id], query, top_k=5, model_type=requested_model)

        if not similar_docs:
            return {
                "success": True,
                "answer": "Kh√¥ng t√¨m th·∫•y n·ªôi dung li√™n quan trong file.",
                "sources": [],
                "user_storage_id": user_storage_id
            }

        # Get retriever for agent
        retriever = await get_or_create_retriever(user_storage_id, model_type=requested_model)
        if not retriever:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o retriever")

        # Use agent to answer with consistent model_type
        agent = SimpleChatAgent(custom_retriever=retriever, model_type=requested_model)
        answer = agent.chat(query)

        sources = [{"content": doc.content[:500], "page": doc.page_range, "score": doc.similarity_score}
                   for doc in similar_docs[:3]]

        return {
            "success": True,
            "answer": answer,
            "sources": sources,
            "user_storage_id": user_storage_id,
            "filename": file_info.filename
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error querying file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")


@router.post("/multi-query", response_model=Dict[str, Any])
async def multi_query_file(request: MultiQueryRequest):
    """Query file v·ªõi nhi·ªÅu c√¢u h·ªèi"""
    try:
        user_storage_id = request.user_storage_id
        queries = request.queries

        if not queries:
            raise HTTPException(status_code=400, detail="Kh√¥ng c√≥ c√¢u h·ªèi")

        file_info = await ocr_service.get_file_info(user_storage_id)
        if not file_info:
            raise HTTPException(status_code=404, detail="File not found")

        if file_info.processing_status != "COMPLETED":
            raise HTTPException(status_code=400, detail=f"File ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω")

        # Determine model_type
        from src.llm.model_manager import model_manager, ModelType
        system_model_type = model_manager.get_model_type()
        requested_model = request.model_type
        
        if not requested_model:
            requested_model = "fayedark" if system_model_type == ModelType.OLLAMA else "gemini"

        retriever = await get_or_create_retriever(user_storage_id, model_type=requested_model)
        if not retriever:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o retriever")

        agent = SimpleChatAgent(custom_retriever=retriever, model_type=requested_model)
        pg_store = get_pg_vector_store()

        results = []
        for query in queries:
            answer = agent.chat(query)
            similar_docs = await pg_store.search_similar([user_storage_id], query, top_k=2, model_type=requested_model)

            results.append({
                "query": query,
                "answer": answer,
                "sources": [doc.content[:300] for doc in similar_docs]
            })

        return {
            "success": True,
            "results": results,
            "user_storage_id": user_storage_id,
            "filename": file_info.filename
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in multi-query: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/file-status/{user_storage_id}", response_model=Dict[str, Any])
async def get_file_status(user_storage_id: str):
    """Get file processing status"""
    file_info = await ocr_service.get_file_info(user_storage_id)

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    chunks = await ocr_service.get_document_chunks(user_storage_id)

    return {
        "success": True,
        "user_storage_id": user_storage_id,
        "filename": file_info.filename,
        "processing_status": file_info.processing_status,
        "chunks_count": len(chunks)
    }


@router.delete("/clear-cache/{user_storage_id}")
async def clear_retriever_cache(user_storage_id: str):
    """Clear retriever cache for a file"""
    if user_storage_id in _retriever_cache:
        del _retriever_cache[user_storage_id]
        return {"success": True, "message": "Cache cleared"}
    return {"success": True, "message": "No cache to clear"}
